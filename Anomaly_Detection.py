# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10WYJbjftzvJmqId_xvG2fVZMVe5lH_zO
"""
'''
# install dependencies: (use cu101 because colab has CUDA 10.1)
!pip install cython pyyaml==5.1

# install detectron2:
!pip install detectron2==0.1.3 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.5/index.html

'''

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import cv2

cap = cv2.VideoCapture(0)
#cv2.rotate(cap,cv2.ROTATE_90_CLOCKWISE)

# Define the codec and create VideoWriter object
fourcc = cv2.VideoWriter_fourcc(*'XVID')
out = cv2.VideoWriter('output.avi',fourcc, 20.0, (640,480))

# Video Capturing started
while(cap.isOpened()):
    ret, frame = cap.read()
    if ret==True:
        frame = cv2.flip(frame,1)

        # write the flipped frame
        out.write(frame)

        cv2.imshow('frame',frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    else:
        break

# Release everything if job is finished
cap.release()
out.release()
cv2.destroyAllWindows()


# In[ ]:





# Commented out IPython magic to ensure Python compatibility.
from PIL import Image
import numpy as np 
import cv2
import requests
import matplotlib.pyplot as plt
# %config InlineBackend.figure_format = 'retina'
 
import torch
from torch import nn
from torchvision.models import resnet50
import torchvision.transforms as T
torch.set_grad_enabled(False);

class DETRdemo(nn.Module):
    
    def __init__(self, num_classes, hidden_dim=256, nheads=8,
                 num_encoder_layers=6, num_decoder_layers=6):
        super().__init__()

        # create ResNet-50 backbone
        self.backbone = resnet50()
        del self.backbone.fc

        # create conversion layer
        self.conv = nn.Conv2d(2048, hidden_dim, 1)

        # create a default PyTorch transformer
        self.transformer = nn.Transformer(
            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)

        # prediction heads, one extra class for predicting non-empty slots
        # note that in baseline DETR linear_bbox layer is 3-layer MLP
        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)
        self.linear_bbox = nn.Linear(hidden_dim, 4)

        # output positional encodings (object queries)
        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))

        # spatial positional encodings
        # note that in baseline DETR we use sine positional encodings
        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))
        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))

    def forward(self, inputs):
        # propagate inputs through ResNet-50 up to avg-pool layer
        x = self.backbone.conv1(inputs)
        x = self.backbone.bn1(x)
        x = self.backbone.relu(x)
        x = self.backbone.maxpool(x)

        x = self.backbone.layer1(x)
        x = self.backbone.layer2(x)
        x = self.backbone.layer3(x)
        x = self.backbone.layer4(x)

        # convert from 2048 to 256 feature planes for the transformer
        h = self.conv(x)

        # construct positional encodings
        H, W = h.shape[-2:]
        pos = torch.cat([
            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),
            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),
        ], dim=-1).flatten(0, 1).unsqueeze(1)

        # propagate through the transformer
        h = self.transformer(pos + 0.1 * h.flatten(2).permute(2, 0, 1),
                             self.query_pos.unsqueeze(1)).transpose(0, 1)
        
        # finally project transformer outputs to class labels and bounding boxes
        return {'pred_logits': self.linear_class(h), 
                'pred_boxes': self.linear_bbox(h).sigmoid()}

detr = DETRdemo(num_classes=91)
state_dict = torch.hub.load_state_dict_from_url(
    url='https://dl.fbaipublicfiles.com/detr/detr_demo-da2a99e9.pth',
    map_location='cpu', check_hash=True)
detr.load_state_dict(state_dict)
detr.eval();

# COCO classes
CLASSES = [
    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',
    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',
    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',
    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',
    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',
    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',
    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',
    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',
    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',
    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',
    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',
    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',
    'toothbrush', 
]
#targated classes to demonstration
targated_classes = ['person', 'car', 'stop sign', 'traffic light', 'knife', 'motorcycle']
#defined anomaly list declaration
Anomaly = [('person','car'),('car','person'),('stop sign','car'),('car','stop sign'),('person','knife'),('knife','person'),('car','traffic light'),('traffic light','car')]
# colors for visualization
COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],
          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]

# standard PyTorch mean-std input image normalization
transform = T.Compose([
    T.Resize(800),
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# for output bounding box post-processing
def box_cxcywh_to_xyxy(x):
    x_c, y_c, w, h = x.unbind(1)
    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),
         (x_c + 0.5 * w), (y_c + 0.5 * h)]
    return torch.stack(b, dim=1)

def rescale_bboxes(out_bbox, size):
    img_w, img_h = size
    b = box_cxcywh_to_xyxy(out_bbox)
    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)
    return b

def detect(im, model, transform):
    # mean-std normalize the input image (batch-size: 1)
    img = transform(im).unsqueeze(0)

    # propagate through the model
    outputs = model(img)

    # keep only predictions with 0.7+ confidence
    probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]
    keep = probas.max(-1).values > 0.7

    # convert boxes from [0; 1] to image scales
    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)
    return probas[keep], bboxes_scaled

# Commented out IPython magic to ensure Python compatibility.
# %%time
# !rm -r frames/*
# !mkdir frames/
import glob, os
if os.path.exists("C:\\Users\\ANSHIKA SHARMA\\Desktop\\frames\\frames") == True :
    test = "frames\\"
        

else:
    directory = "frames\\"
    parent_dir = "C:\\Users\\'ANSHIKA SHARMA'\\Desktop\\frames\\"
    path = os.path.join(parent_dir, directory)
    os.mkdir(path)
    print("Directory '% s' created" % directory)    

import math
'''
# #specify path to video
video = "/content/2.mp4"
# 
##capture video
cap = cv2.VideoCapture(video)
'''

cnt = 0
videoFile = "3.mp4"
imagesFolder = ".//frames"
cap = cv2.VideoCapture(videoFile)
frameRate = cap.get(5) #frame rate
while(cap.isOpened()):
    frameId = cap.get(1) #current frame number
    ret, frame = cap.read()
    if (ret != True):
        break
    if (frameId % math.floor(frameRate) == 0):
        filename = imagesFolder+"//"+str(cnt)+ ".png"
        cv2.imwrite(filename, frame)
        cnt=cnt+1
cap.release()
print("Done")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# #url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
# #im = Image.open(requests.get(url, stream=True).raw)
#img = cv2.imread("frames/30.png")
# 
im = Image.open("frames/2.png")
scores, boxes = detect(im, detr, transform)
# 
# 
import os
import re
# 
names=os.listdir('frames/')
#names.sort(key=lambda f: int(re.sub('\D', '', f)))
# 
# 
numbering = []
for i in range(len(names)):
    element = names[i]
    temp = element.split('.')
    numbering.append(temp)

#Global List declaration for collecting coordinates
#function to encode each predicted class
def encode_class(x,counter):
  temp = x
  if temp not in lst:
      lst.append(temp)
      value = temp + '_' + str(counter)

  else:
    value = temp + '_' + str(counter)
  
  return value

# Python code to count the number of occurrences 
def countX(lst, x): 
    count = 0
    for ele in lst: 
        if (ele == x): 
            count = count + 1
    return count 

#function to encode the list of all identified classes 
def encoding_classes(my_identification_classes):
  rows = len(my_identification_classes) 
  for i in range(rows):
    temp1 = my_identification_classes[i]
    count = countX(my_identification_classes , temp1) 
  
    if temp1 not in lst:
      lst.append(temp1)
      col = []
      if count == 1:
        for k in range(count):
          value = temp1 + "_" + str(k)
          arr.append(value)
          
      else:
        for k in range(count-1):
          value = temp1 + "_" + str(k)
          arr.append(value)       
          
    else:
      continue
  my_identification_classes_refined = list(arr)
  return my_identification_classes_refined 

#function to calculate the mid_point of each image
def midpoint(xmin,ymin, xmax, ymax):
	return ((xmax + xmin) * 0.5, (ymax + ymin) * 0.5)


#function to calculate all the required resultant lists 
def collecting_result_lists(name, prob, boxes):
    counter = 0 
    img = cv2.imread('frames/'+name) 
    plt.figure(figsize=(16,10))
    #plt.imshow(img)
    #ax = plt.gca()
    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), COLORS * 100):
        cX , cY=  midpoint(xmin, ymin,xmax,ymax)
        center_pt = (cX,cY)
        centers.append(center_pt)
        cl = p.argmax()
        #ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,fill=False, color=c, linewidth=3))
        #text = f'{CLASSES[cl]}: {p[cl]:0.2f}'
        #ax.text(xmin, ymin, text, fontsize=25,bbox=dict(facecolor='white',alpha=0.5))
        #my lists to collect identified class and coordinates data
        for i in range(len(targated_classes)):
          temp_class = targated_classes[i]
          Class_val = CLASSES[cl]
          if (Class_val == temp_class):
            my_identification_class_coordinates.append((CLASSES[cl],p[cl],xmin,ymin,xmax,ymax))
            my_identification_classes.append(CLASSES[cl])
            counter = counter +1 
            temp = encode_class(CLASSES[cl],counter)
            my_identification_classes_encoded_coordinates.append((temp,p[cl],xmin,ymin,xmax,ymax))
            classes_info.append(temp)
          else:
            continue


    my_image_collection.append(my_identification_classes_encoded_coordinates)
    my_image_classes_info.append(classes_info)
    All_image_centers.append(centers)

       
#collecting_result_lists(im, scores, boxes)

my_identification_class_coordinates =  []
my_identification_classes =  []
Final_Classes = []
Final_Predicted_Anomaly = []
my_identification_classes_encoded_coordinates = []
my_identification_classes_encoded = []
centers=[] 
index = []
lst = []
All_image_centers = []
my_image_collection = []
classes_info = []
my_image_classes_info = []

try:
  _ = [collecting_result_lists(names[i], scores, boxes) for i in range(len("frames/"))]
except IndexError:
  print("Wait it is processing!!!!!")

#my_image_collection
#my_image_classes_info
#len(my_image_classes_info)

#Function for finding all possible combinations of the encoded classes

import itertools
def finding_possible_combinations(element):
  for L in range(3):
    for i in itertools.permutations(element, L):
      Final_Classes.append(i)
      temp = Final_Classes    
  Final_Classes_all_images.append(temp)

#Final_Classes = finding_possible_combinations(my_identification_classes_encoded)
#print(Final_Classes)

Final_Classes_all_images = []
_ = [finding_possible_combinations(my_image_classes_info[i]) for i in range(len(my_image_classes_info))]

#Final_Classes_all_images

#Final_Classes_all_images

#Function to filter the classes with two possible categories 
def Filter(Final_Classes):
  for i in range(len(Final_Classes)):
    temp = Final_Classes[i]
    if len(temp) == 2:
      arr.append(temp)
    else:
      continue
  Final_Classes_filtered.append(arr)

#Final_Classes_filtered = Filter(Final_Classes)
#Final_Classes_filtered

arr = []
Final_Classes_filtered = []
_ = [Filter(Final_Classes_all_images[i]) for i in range(len(Final_Classes_all_images))]

#Final_Classes_filtered

def unique(list1):    
    # insert the list to the set 
    list_set = set(list1) 
    # convert the set to the list 
    unique_list = (list(list_set))
    if unique_list is None:
      print("returned list is None") 
    else:
      for x in unique_list: 
        arr.append(x)
    final.append(arr)


def Not_None(final):
  Not_none_values = filter(None,final)
  list_of_values = list(Not_none_values)
  Final_Refined = set(list_of_values)
  Final_Refined = list(Final_Refined)
  Final_Refined_all_img_have_redundent.append(Final_Refined)

arr = []
final = []
Final_Refined_all_img_have_redundent = []
_ = [unique(Final_Classes_filtered[i]) for i in range(len(Final_Classes_filtered))]

_ = [Not_None(final[i]) for i in range(len(final))]

#my_identification_classes_encoded
#centers
#Final_Classes_filtered 

import math
#Function to calculate the distance between two coordinates 
def calculateDistance(x1,y1,x2,y2):
  dist = math.sqrt(((x2 - x1)**2) + ((y2 - y1)**2))
  return dist


def finding_the_list_of_distances(Final_class,counter):
  lst1 = Final_class
  cls_lst = my_image_classes_info[counter]
  center_lst = All_image_centers[counter]
  for i in range(len(lst1)):
    combination01 = lst1[i]
    first_class = combination01[0]
    second_class = combination01[1]
    index1 = cls_lst.index(first_class)
    index2 = cls_lst.index(second_class)

    p1 = center_lst[index1]
    p2 = center_lst[index2]

    x1 = p1[0]
    y1 = p1[1]

    x2 = p2[0]
    y2 = p2[1]
  
    distance = calculateDistance(x1, y1, x2, y2)
    if distance == 0.0 :
      continue
    else:
      List_of_Distances.append(distance)

  List_of_Distances_all_images.append(List_of_Distances)

List_of_Distances = []
List_of_Distances_all_images = []
indices = []
arr = []
_ = [finding_the_list_of_distances(Final_Refined_all_img_have_redundent[i],i) for i in range(len(Final_Refined_all_img_have_redundent))]

#Final_Refined_all_img

def Filtering_redundant_objects(element):
  temp_lst = element
  for i in range(len(temp_lst)):
    val = temp_lst[i]
    first = val[0]
    second = val[1]
    if (first == second) :
      continue
    else:
      arr.append(val)

  Final_Refined_all_img.append(arr)

arr = []
Final_Refined_all_img = []
_ = [Filtering_redundant_objects(Final_Refined_all_img_have_redundent[i]) for i in range(len(Final_Refined_all_img_have_redundent))]

#Final_Refined_all_img

#code to access the minimum distance

def minimum(List_of_Distances, Final_Classes_filtered): 
    # inbuilt function to find the position of minimum  
    value = List_of_Distances
    min_pos = value.index(min(value))
    Class_predicted  = list(Final_Classes_filtered[min_pos])  
    class_pred.append(Class_predicted)
    value_of_distances.append(value)


#output = minimum(List_of_Distances, Final_Classes_filtered)
#print(output)

class_pred = []
value_of_distances = []
_ = [minimum(List_of_Distances_all_images[i], Final_Refined_all_img[i]) for i in range(len(List_of_Distances_all_images))]

#print(class_pred,value_of_distances)

#len(value_of_distances)

#code to decode the combination with minimum distance

def decode_class(output,key):
  temp01 = tuple(output)
  first_class = temp01[0]
  second_class = temp01[1]
  temp_lst = first_class.split('_')
  temp_lst1 = second_class.split('_')
  value1 = temp_lst[0]
  value2 = temp_lst1[0]
  decoded_class01[temp01] = (value1,value2)
  temp = decoded_class01
  Final_Decoded_classes.append(temp)
  
  

#decoding the combination with minimum distance
#decoded_class = decode_class(output)
#print(decoded_class)

decoded_class01 = {}
Final_Decoded_classes = []
_ = [decode_class(class_pred[i],i) for i in range(len(class_pred))]

#decoded_class01
Final_Decoded_classes

#calculating list of predicted classes 
def calculate_list_of_predicted_class(decoded_class,imgval):
  temp01 = list(decoded_class.values())
  temp = temp01[0]
  key01 = list(decoded_class.keys())
  key = key01[0]
  for i in range(len(Anomaly)):
    if temp == Anomaly[i]:
      x = Anomaly[i]
      if   ((x==('person','car')) or (x==('car','person'))):
        image_val.append(imgval)
        Final_Anomaly.append(key)
        break
      elif ((x==('stop sign','car')) or (x==('car','stop sign'))):
        image_val.append(imgval)
        Final_Anomaly.append(key)
        break
      elif ((x==('person','knife')) or (x==('knife','person'))):
        image_val.append(imgval)
        Final_Anomaly.append(key)
        break
      elif ((x==('car','traffic light')) or (x==('traffic light','car'))):
        image_val.append(imgval)
        Final_Anomaly.append(key)
        break
      else: 
        print("There is no Anomaly !!!!")
      break

    else:
      image_val.append(imgval)

#predicted_class = calculate_list_of_predicted_class(decoded_class)
#print(predicted_class)

Final_Anomaly = []
image_val = []
_ = [calculate_list_of_predicted_class(Final_Decoded_classes[i],i) for i in range(len(Final_Decoded_classes))]


def final_requirement(Final_Anomaly, my_image_classes_info,my_image_collection,ind_val):
  x = my_image_collection[ind_val]
  y = my_image_classes_info[ind_val]
  for i in range(len(x)):
    temp = x[i]
    anomaly = Final_Anomaly[0]
    cl1 = anomaly[0]
    cl2 = anomaly[1]
    for j in range(len(y)):
      if cl1 == y[j]:
        ind = x[j]
        val1 =  (ind[1:])
        break
      else:
        continue

    for k in range(len(y)):
      if cl2 == y[k]:
        ind1 = x[k]
        val2 =  (ind1[1:])
        break
      else:
        continue

    final_coordinate_dictio[cl1] = val1
    final_coordinate_dictio[cl2] = val2

def Resize_img(img):
  scale_percent = 60 # percent of original size
  width = int(img.shape[1] * scale_percent / 100)
  height = int(img.shape[0] * scale_percent / 100)
  dim = (width, height)
  # resize image
  resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)
  return resized


def return_values(ordinates):
  x1 =  float(ordinates[0])
  y1 =  float(ordinates[1])
  x2 =  float(ordinates[2])
  y2 =  float(ordinates[3])
  return x1,y1,x2,y2

def plot_results(name, scores, boxes):
    img = cv2.imread('C://Users//ANSHIKA SHARMA//Desktop//frames//'+name) 
    plt.figure(figsize=(16,10))
    plt.imshow(img)
    ax = plt.gca()
    color = [[0.768, 0.000, 0.000]]
    for (xmin, ymin, xmax, ymax), c in zip( boxes.tolist(), color * 100):
        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,fill=False, color=c, linewidth=3))
    plt.show()
  

if len(Final_Anomaly)==0:
  print("There is no Anomaly!!!!")

else:
  final_coordinate_dictio = {}
  ind_val = image_val[0]
  final_requirement(Final_Anomaly, my_image_classes_info,my_image_collection,ind_val)
  name = "0.png"
  output_Anomaly = f"There is an Anomaly with a combination of { Classes }  "
  print(output_Anomaly)
  plot_results(name, scores, boxes)
  Classes = list(final_coordinate_dictio.keys())
    

m=my_identification_class_coordinates[0][1].numpy()
print(f"{m:0.2f}")