# -*- coding: utf-8 -*-
"""Copy of Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PPEbTuPGrw_JiOsfdyXX6kumLh5ZqN4B
"""
'''
# install dependencies: (use cu101 because colab has CUDA 10.1)
!pip install cython pyyaml==5.1

# install detectron2:
!pip install detectron2==0.1.3 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.5/index.html

'''

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import cv2

cap = cv2.VideoCapture(0)

# Define the codec and create VideoWriter object
fourcc = cv2.VideoWriter_fourcc(*'XVID')
out = cv2.VideoWriter('output.avi',fourcc, 20.0, (640,480))

# Video Capturing started
while(cap.isOpened()):
    ret, frame = cap.read()
    if ret==True:
        frame = cv2.flip(frame,1)

        # write the flipped frame
        out.write(frame)

        cv2.imshow('frame',frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    else:
        break

# Release everything if job is finished
cap.release()
out.release()
cv2.destroyAllWindows()


# In[ ]:





# import some common libraries
import numpy as np
import cv2
import random
#from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt

# Commented out IPython magic to ensure Python compatibility.
from PIL import Image
import requests
import matplotlib.pyplot as plt
# %config InlineBackend.figure_format = 'retina'
 
import torch
from torch import nn
from torchvision.models import resnet50
import torchvision.transforms as T
torch.set_grad_enabled(False);

class DETRdemo(nn.Module):
    
    def __init__(self, num_classes, hidden_dim=256, nheads=8,
                 num_encoder_layers=6, num_decoder_layers=6):
        super().__init__()

        # create ResNet-50 backbone
        self.backbone = resnet50()
        del self.backbone.fc

        # create conversion layer
        self.conv = nn.Conv2d(2048, hidden_dim, 1)

        # create a default PyTorch transformer
        self.transformer = nn.Transformer(
            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)

        # prediction heads, one extra class for predicting non-empty slots
        # note that in baseline DETR linear_bbox layer is 3-layer MLP
        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)
        self.linear_bbox = nn.Linear(hidden_dim, 4)

        # output positional encodings (object queries)
        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))

        # spatial positional encodings
        # note that in baseline DETR we use sine positional encodings
        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))
        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))

    def forward(self, inputs):
        # propagate inputs through ResNet-50 up to avg-pool layer
        x = self.backbone.conv1(inputs)
        x = self.backbone.bn1(x)
        x = self.backbone.relu(x)
        x = self.backbone.maxpool(x)

        x = self.backbone.layer1(x)
        x = self.backbone.layer2(x)
        x = self.backbone.layer3(x)
        x = self.backbone.layer4(x)

        # convert from 2048 to 256 feature planes for the transformer
        h = self.conv(x)

        # construct positional encodings
        H, W = h.shape[-2:]
        pos = torch.cat([
            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),
            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),
        ], dim=-1).flatten(0, 1).unsqueeze(1)

        # propagate through the transformer
        h = self.transformer(pos + 0.1 * h.flatten(2).permute(2, 0, 1),
                             self.query_pos.unsqueeze(1)).transpose(0, 1)
        
        # finally project transformer outputs to class labels and bounding boxes
        return {'pred_logits': self.linear_class(h), 
                'pred_boxes': self.linear_bbox(h).sigmoid()}

detr = DETRdemo(num_classes=91)
state_dict = torch.hub.load_state_dict_from_url(
    url='https://dl.fbaipublicfiles.com/detr/detr_demo-da2a99e9.pth',
    map_location='cpu', check_hash=True)
detr.load_state_dict(state_dict)
detr.eval();

# COCO classes
CLASSES = [
    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',
    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',
    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',
    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',
    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',
    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',
    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',
    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',
    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',
    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',
    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',
    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',
    'toothbrush', 
]

#targated classes to demonstration
targated_classes = ['person', 'car', 'stop sign', 'traffic light', 'knife', 'motorcycle']

#defined anomaly list declaration
Anomaly = [('person','car'),('car','person'),('person','stop sign'),('stop sign','person'),('person','knife'),('knife','person')]
# colors for visualization
COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],
          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]

Anomaly

# standard PyTorch mean-std input image normalization
transform = T.Compose([
    T.Resize(800),
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# for output bounding box post-processing
def box_cxcywh_to_xyxy(x):
    x_c, y_c, w, h = x.unbind(1)
    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),
         (x_c + 0.5 * w), (y_c + 0.5 * h)]
    return torch.stack(b, dim=1)

def rescale_bboxes(out_bbox, size):
    img_w, img_h = size
    b = box_cxcywh_to_xyxy(out_bbox)
    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)
    return b

def detect(im, model, transform):
    # mean-std normalize the input image (batch-size: 1)
    img = transform(im).unsqueeze(0)

    # propagate through the model
    outputs = model(img)

    # keep only predictions with 0.7+ confidence
    probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]
    keep = probas.max(-1).values > 0.7

    # convert boxes from [0; 1] to image scales
    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)
    return probas[keep], bboxes_scaled

# Commented out IPython magic to ensure Python compatibility.
# %%time
# !rm -r frames/*
# !mkdir frames/
# import math
# '''
import os

# You should change 'test' to your preferred folder.
MYDIR = ("test")
CHECK_FOLDER = os.path.isdir(MYDIR)

# If folder doesn't exist, then create it.
if not CHECK_FOLDER:
    os.makedirs(MYDIR)
    print("created folder : ", MYDIR)

else:
    print(MYDIR, "folder already exists.")
# #specify path to video
video = "output.avi"
# 
# #capture video
cap = cv2.VideoCapture(video)
# '''
import cv2
cnt = 0
videoFile = "3.mp4"
imagesFolder = "test"
cap = cv2.VideoCapture(videoFile)
frameRate = cap.get(5) #frame rate
while(cap.isOpened()):
    frameId = cap.get(1) #current frame number
    ret, frame = cap.read()
    if (ret != True):
        break
    if (frameId % math.floor(frameRate) == 0):
        filename = imagesFolder+"/"+str(cnt)+ ".png"
        cv2.imwrite(filename, frame)
        cnt=cnt+1
cap.release()
print("Done")

#frame rate of a video
FPS=cap.get(cv2.CAP_PROP_FPS)
FPS=cap.set(cv2.CAP_PROP_FPS,15)
FPS=cap.get(cv2.CAP_PROP_FPS)
print(FPS)

#url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
#im = Image.open(requests.get(url, stream=True).raw)
#img = cv2.imread("frames/30.png")
im = Image.open("test\\0.png")
scores, boxes = detect(im, detr, transform)
#im

#Global List declaration for collecting coordinates
my_identification_class_coordinates =  []
my_identification_classes =  []
Final_Classes = []
Final_Predicted_Anomaly = []
my_identification_classes_encoded_coordinates = []
my_identification_classes_encoded = []
centers=[] 
index = []
lst = []

#function to encode each predicted class
def encode_class(x,counter):
  temp = x
  if temp not in lst:
      lst.append(temp)
      value = temp + '_' + str(counter)

  else:
    value = temp + '_' + str(counter)
  
  return value

# Python code to count the number of occurrences 
def countX(lst, x): 
    count = 0
    for ele in lst: 
        if (ele == x): 
            count = count + 1
    return count 

#function to encode the list of all identified classes 
def encoding_classes(my_identification_classes):
  rows = len(my_identification_classes) 
  for i in range(rows):
    temp1 = my_identification_classes[i]
    count = countX(my_identification_classes , temp1) 
  
    if temp1 not in lst:
      lst.append(temp1)
      col = []
      if count == 1:
        for k in range(count):
          value = temp1 + "_" + str(k)
          arr.append(value)
          
      else:
        for k in range(count-1):
          value = temp1 + "_" + str(k)
          arr.append(value)       
          
    else:
      continue
  my_identification_classes_refined = list(arr)
  return my_identification_classes_refined 

#function to calculate the mid_point of each image
def midpoint(xmin,ymin, xmax, ymax):
	return ((xmax + xmin) * 0.5, (ymax + ymin) * 0.5)

#function to calculate all the required resultant lists 
def collecting_result_lists(pil_img, prob, boxes):
    counter = 0 
    #plt.figure(figsize=(16,10))
    #plt.imshow(pil_img)
    #ax = plt.gca()
    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), COLORS * 100):
        cX , cY=  midpoint(xmin, ymin,xmax,ymax)
        center_pt = (cX,cY)
        centers.append(center_pt)
        cl = p.argmax()
        #ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,fill=False, color=c, linewidth=3))
        #text = f'{CLASSES[cl]}: {p[cl]:0.2f}'
        #ax.text(xmin, ymin, text, fontsize=25,bbox=dict(facecolor='white',alpha=0.5))
        #my lists to collect identified class and coordinates data
        for i in range(len(targated_classes)):
          temp_class = targated_classes[i]
          Class_val = CLASSES[cl]
          if (Class_val == temp_class):
            my_identification_class_coordinates.append((CLASSES[cl],p[cl],xmin,ymin,xmax,ymax))
            my_identification_classes.append(CLASSES[cl])
            counter = counter +1 
            temp = encode_class(CLASSES[cl],counter)
            my_identification_classes_encoded.append(temp)
            my_identification_classes_encoded_coordinates.append((temp,p[cl],xmin,ymin,xmax,ymax))
            img_info = my_identification_classes_encoded_coordinates
          else:
            continue   
       
collecting_result_lists(im, scores, boxes)

#my_identification_class_coordinates
my_identification_classes_encoded

#Function for finding all possible combinations of the encoded classes
import itertools
def finding_possible_combinations(my_identification_classes_encoded):
  for L in range(3):
    for i in itertools.permutations(my_identification_classes_encoded, L):
      Final_Classes.append(i) 

  return Final_Classes 

Final_Classes = finding_possible_combinations(my_identification_classes_encoded)
#print(Final_Classes)

arr = []
#Function to filter the classes with two possible categories 
def Filter(Final_Classes):
  for i in range(len(Final_Classes)):
    temp = Final_Classes[i]
    if len(temp) == 2:
      arr.append(temp)
    else:
      continue
  Final_Classes_filtered = arr
  return Final_Classes_filtered

Final_Classes_filtered = Filter(Final_Classes)
Final_Classes_filtered



'''
def unique(list1):    
    # insert the list to the set 
    list_set = set(list1) 
    # convert the set to the list 
    unique_list = (list(list_set))
    if unique_list is None:
      print("returned list is None") 
    else:
      for x in unique_list: 
        print(x)
#Final_Predicted_Anomaly_Refined = list(unique(Final_Predicted_Anomaly))
Not_none_values = filter(None,Final_Predicted_Anomaly)
list_of_values = list(Not_none_values)
Final_Predicted_Anomaly_Refined = set(list_of_values)
#print(list(mylist))

Final_Predicted_Anomaly_Refined = list(Final_Predicted_Anomaly_Refined)
Final_Predicted_Anomaly_Refined

'''

#my_identification_classes_encoded
#centers
#Final_Classes_filtered 
List_of_Distances = []
import math
#Function to calculate the distance between two coordinates 
def calculateDistance(x1,y1,x2,y2):
  dist = math.sqrt(((x2 - x1)**2) + ((y2 - y1)**2))
  return dist

def finding_the_list_of_distances(Final_Classes_filtered):
  for i in range(len(Final_Classes_filtered)):
    combination01 = Final_Classes_filtered[i]  # ('umbe..', 'car')
    first_class = combination01[0]
    second_class = combination01[1]
    index1 = my_identification_classes_encoded.index(first_class)
    index2 = my_identification_classes_encoded.index(second_class)

    p1 = centers[index1]
    p2 = centers[index2]

    x1 = p1[0]
    y1 = p1[1]

    x2 = p2[0]
    y2 = p2[1]
  
    distance = calculateDistance(x1, y1, x2, y2)
    List_of_Distances.append(distance)

  return List_of_Distances

List_of_Distances = finding_the_list_of_distances(Final_Classes_filtered)

''' 
#print(first_class)
print(len(Final_Classes_filtered))
print(len(List_of_Distances))
print(p1,p2)
'''

#code to access the minimum distance
def minimum(List_of_Distances, Final_Classes_filtered): 
    # inbuilt function to find the position of minimum  
    value = min(List_of_Distances)
    min_pos = List_of_Distances.index(min(List_of_Distances))
    Class_predicted  = Final_Classes_filtered[min_pos]
    Final_Anomaly.append(Class_predicted)
    return (Class_predicted, value )

Final_Anomaly = []
output = minimum(List_of_Distances, Final_Classes_filtered)
print(output)

#code to decode all possible combinations of the filtered final classes

def decoding_classes(Final_Classes_filtered):
  for i in range(len(Final_Classes_filtered)):
    temp01 = Final_Classes_filtered[i]
    first_class = temp01[0]
    second_class = temp01[1]
    temp_lst = first_class.split('_')
    temp_lst1 = second_class.split('_')
    value1 = temp_lst[0]
    value2 = temp_lst1[0]
    my_identification_classes_refined_dictio[temp01] = (value1,value2)

my_identification_classes_refined_dictio = {}
#decoding the complete list of all possible combinations 
decoding_classes(Final_Classes_filtered)

my_identification_classes_refined_dictio

#code to decode the combination with minimum distance
decoded_class = {}
def decode_class(output):
  temp01 = output[0]
  first_class = temp01[0]
  second_class = temp01[1]
  temp_lst = first_class.split('_')
  temp_lst1 = second_class.split('_')
  value1 = temp_lst[0]
  value2 = temp_lst1[0]
  decoded_class[temp01] = (value1,value2)
  return decoded_class

#decoding the combination with minimum distance
decoded_class = decode_class(output)
print(decoded_class)

predicted_class = []
#calculating list of predicted classes 
def calculate_list_of_predicted_class(decoded_class):
  temp = list(decoded_class.keys())
  print(temp)
  temp_a = temp[0][0]
  temp_b = temp[0][1]
  predicted_class.append(temp_a)
  predicted_class.append(temp_b)

  return predicted_class

predicted_class = calculate_list_of_predicted_class(decoded_class)
print(predicted_class)

final_coordinate_dictio = {}
pred_class_values = []
lst = []
def final_requirement(predicted_class,my_identification_classes_encoded_coordinates ):
  for i in range(len(my_identification_classes_encoded_coordinates)):
    temp = my_identification_classes_encoded_coordinates[i]
    temp_key = (temp[0])
    temp_value = (temp[1:])
    final_coordinate_dictio[temp_key] = temp_value
    if temp_key not in lst:
      lst.append(temp_key)
      predicted_class.append(temp_key)
      pred_class_values.append(temp_value)

    else:
      continue
      
  return (predicted_class, pred_class_values)


predicted_class, pred_class_values = final_requirement(predicted_class,my_identification_classes_encoded_coordinates )

final_coordinate_dictio 
#pred_class_values

#predicted_class

pred_class_values

#function to calculate all the required resultant lists 
def plot_results(pil_img, prob, boxes,predicted_class, pred_class_values ):
    counter = 0 
    plt.figure(figsize=(16,10))
    plt.imshow(pil_img)
    ax = plt.gca()
    i=0
    for  Class, (prob ,xmin, ymin, xmax, ymax), c in zip(predicted_class, pred_class_values, COLORS * 100):
        
        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,
                                   fill=False, color=c, linewidth=3))
        
        text = f'{predicted_class[i]}: {prob:0.2f}'
        ax.text(xmin, ymin, text, fontsize=25,bbox=dict(facecolor='white',alpha=0.5))
        i=i+1
              
#img = plot_results(im, scores, boxes, predicted_class, pred_class_values)

import os
import re

names=os.listdir('test\\')
names.sort(key=lambda f: int(re.sub('\D', '', f)))

def unique(list1): 
  
    # intilize a null list 
    unique_list = [] 
      
    # traverse for all elements 
    for x in list1: 
        # check if exists in unique_list or not 
        if x not in unique_list: 
            unique_list.append(x) 
    # print list 
    for x in unique_list: 
        print(x)

    return unique_list

predicted_class_final = unique(predicted_class)

pred_class_values

#predicted_class_final

def add_boxex(img,predicted_class_final, pred_class_values):
  for i in range(len(predicted_class_final)):
    clss = predicted_class_final[i]
    temp = pred_class_values[i]
    prob = temp[0]
    x1 = int(temp[1])
    y1 = int(temp[2])
    x2 = int(temp[3])
    y2 = int(temp[4])
     
    print(x1,y1,x2,y2)
    _ = cv2.rectangle(img, (x1, y1), (x2, y2), (255,0,0), 2)
  return img



def finding_anomaly(name, thresh):

  img = cv2.imread('test\\'+name)
  collecting_result_lists(im, scores, boxes)
  Final_Classes = finding_possible_combinations(my_identification_classes_encoded)
  Final_Classes_filtered = Filter(Final_Classes)
  List_of_Distances =  finding_the_list_of_distances(Final_Classes_filtered)
  output = minimum(List_of_Distances, Final_Classes_filtered)
  decoded_class = decode_class(output)
  predicted_class = calculate_list_of_predicted_class(decoded_class)
  predicted_class, pred_class_values = final_requirement(predicted_class,my_identification_classes_encoded_coordinates )
  plot_results(im, scores, boxes, predicted_class, pred_class_values)
  predicted_class_final = unique(predicted_class)
  image =  add_boxex(img,predicted_class_final, pred_class_values)
  try:
      cv2.imwrite(r'test\\'+name,image)
      
  except:
      print("Don't worry it will process it further!!!!!")
      
  cv2.imwrite('test\\'+name,image)
  return 0

from tqdm import tqdm
thresh=100
_ = [finding_anomaly(names[i], thresh) for i in tqdm(range(len("names"))) ]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
import os
import re

names=os.listdir('test\\')
names.sort(key=lambda f: int(re.sub('\D', '', f)))

frame_array=[]

for i in range(len(names)):
    img = cv2.imread('test\\'+names[i])
    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    height, width, layers = img.shape
    size = (width,height)
    frame_array.append(img)

out = cv2.VideoWriter('sample_output01.mp4',cv2.VideoWriter_fourcc(*'DIVX'), 25, size)
print("output video created")
for i in range(len(frame_array)):
    out.write(frame_array[i])
out.release()

m=my_identification_class_coordinates[0][1].numpy()
print(f"{m:0.2f}")

import pyglet
 
vidPath = 'sample_output01.mp4'
window= pyglet.window.Window()
player = pyglet.media.Player()
source = pyglet.media.StreamingSource()
MediaLoad = pyglet.media.load(vidPath)
 
player.queue(MediaLoad)
player.play()
 
 
@window.event
def on_draw():
    if player.source and player.source.video_format:
        player.get_texture().blit(50,50)
 
 
 
pyglet.app.run()